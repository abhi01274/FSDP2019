{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Pokeman.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPIqe4ojAaDn",
        "colab_type": "text"
      },
      "source": [
        "http://www.jessicayung.com/using-generators-in-python-to-train-machine-learning-models/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovS65n_0fzvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeQ2D8nGf8Vl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho0_Z2BfEEtU",
        "colab_type": "code",
        "outputId": "21bc676c-1be3-495a-b35b-20f6528f435a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBqjGNkWdj7H",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7cgWj4Ifhjp",
        "colab_type": "code",
        "outputId": "1c0c6ab5-8e30-45dd-e580-e04135390cf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "print (os.getcwd())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2MrHUfOEKCo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53e10a31-1793-4582-9e02-cbd8a7493e77"
      },
      "source": [
        "# Part 1 - Building the CNN\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbKFBon9ESGw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "441b0fba-ddb4-4186-99a8-65c2f25234dc"
      },
      "source": [
        "# Initialising the CNN\n",
        "classifier = Sequential()\n",
        "\n",
        "# Step 1 - Convolution\n",
        "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
        "\n",
        "# Step 2 - Pooling\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Adding a second convolutional layer\n",
        "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Step 3 - Flattening\n",
        "classifier.add(Flatten())\n",
        "\n",
        "# Step 4 - Full connection\n",
        "classifier.add(Dense(units = 128, activation = 'relu'))\n",
        "classifier.add(Dense(units = 3, activation = 'softmax'))\n",
        "\n",
        "# Compiling the CNN\n",
        "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0614 11:32:32.803416 140234393319296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0614 11:32:32.842745 140234393319296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0614 11:32:32.849151 140234393319296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0614 11:32:32.873792 140234393319296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0614 11:32:32.924608 140234393319296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0614 11:32:32.949482 140234393319296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc9EDHuyd2Tf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b289db7a-8c0c-4cd8-d32e-332a3a976ea3"
      },
      "source": [
        "pwd\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq_AO-WQEdv1",
        "colab_type": "code",
        "outputId": "f54aa186-7d2c-4ddb-d35d-5258d6a31d3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "# Part 2 - Fitting the CNN to the images\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('drive/My Drive/dataset/pokemon_train',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('drive/My Drive/dataset/pokemon_test',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'categorical')\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 550 images belonging to 3 classes.\n",
            "Found 168 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhKgHYiFYxWE",
        "colab_type": "code",
        "outputId": "e2b02fcd-72d1-40e4-a79f-b784f3297ee1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(training_set)\n",
        "type(train_datagen)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "keras.preprocessing.image.ImageDataGenerator"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adRQ0gspM58u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs3z4qmgE1th",
        "colab_type": "code",
        "outputId": "b5127ef6-4386-4b4f-90b3-fa35ff641de6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "classifier.fit_generator(training_set,\n",
        "                         steps_per_epoch = 100,\n",
        "                         epochs = 5,#25\n",
        "                         validation_data = test_set,\n",
        "                         validation_steps = 200)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0614 11:39:07.501942 140234393319296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0614 11:39:07.570382 140234393319296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "  1/100 [..............................] - ETA: 42:03 - loss: 1.1291 - acc: 0.2812"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:914: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
            "  'to RGBA images')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 513s 5s/step - loss: 0.5212 - acc: 0.7863 - val_loss: 0.2580 - val_acc: 0.9048\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 243s 2s/step - loss: 0.2693 - acc: 0.9035 - val_loss: 0.2792 - val_acc: 0.8871\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 237s 2s/step - loss: 0.1755 - acc: 0.9414 - val_loss: 0.2683 - val_acc: 0.9162\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 236s 2s/step - loss: 0.1194 - acc: 0.9539 - val_loss: 0.2364 - val_acc: 0.9406\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 241s 2s/step - loss: 0.0932 - acc: 0.9697 - val_loss: 0.1933 - val_acc: 0.9401\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8ac09fd6a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PKj6cvM4EmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9zN7erD8GqL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0770aedf-f69d-4c86-84d2-968837c1e4d0"
      },
      "source": [
        "pwd\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT_uzEIG4d2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ti = image.load_img('drive/My Drive/download1.jpg',target_size=(64,64))\n",
        "ti = image.img_to_array(ti)\n",
        "ti = np.expand_dims(ti, axis=0)\n",
        "ti = ti.reshape(1,64,64,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruPbZFJt5AhH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3da1aaf-cb0e-4b29-a716-995fed5be433"
      },
      "source": [
        "result = classifier.predict_classes(ti)\n",
        "print(result)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHAtDoM2P6Er",
        "colab_type": "text"
      },
      "source": [
        "## Using the pretrained model: VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uyazfWbMBbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import VGG16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_97RbbAjrLP5",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljseih9Cxp9N",
        "colab_type": "code",
        "outputId": "cafcaded-5b77-4de3-fd10-1f4fea68d658",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wwE9p4BOg0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdnEJ-c5I25J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cASZ4vNM8_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "import os\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(3, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1tbESS5ODt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.trainable = False #do not train the conv-base"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Urx8mGqnONmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_dir = '/content/drive/My Drive/dataset/'\n",
        "train_dir = os.path.join(base_dir, 'training_set')\n",
        "\n",
        "test_dir = os.path.join(base_dir, 'test_set')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnRMbUIcO0mS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=40, width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode='nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255) #rescale factor is feature scalling, convert values in the range of 0-1 before applying any processing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgGVkhW_PJMv",
        "colab_type": "code",
        "outputId": "3afd8418-de2c-4cbd-c27e-9bab5540255d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(train_dir, \n",
        "                                                    target_size=(150, 150),\n",
        "                                                    batch_size=20,\n",
        "                                                    class_mode='categorical')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 445 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBQ5LhuzPdqL",
        "colab_type": "code",
        "outputId": "d315b1db-1a33-455c-8dee-8994ecb77381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_generator = test_datagen.flow_from_directory(test_dir,\n",
        "                                                  target_size=(150, 150),\n",
        "                                                  batch_size=20,\n",
        "                                                  class_mode='categorical')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 238 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL18ENzzPyfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3HemyIVQEg2",
        "colab_type": "code",
        "outputId": "a2d30a3c-bc43-4425-a341-3adf1249ea0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1078
        }
      },
      "source": [
        "history = model.fit_generator(train_generator, steps_per_epoch=100,\n",
        "                              epochs=30,\n",
        "                              validation_data=test_generator,\n",
        "                              validation_steps=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "  4/100 [>.............................] - ETA: 2:26 - loss: 1.0805 - acc: 0.4000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:914: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
            "  'to RGBA images')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 101s 1s/step - loss: 0.2541 - acc: 0.8885 - val_loss: 0.0936 - val_acc: 0.9667\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 91s 912ms/step - loss: 0.0694 - acc: 0.9800 - val_loss: 0.0311 - val_acc: 0.9879\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 92s 919ms/step - loss: 0.0219 - acc: 0.9940 - val_loss: 0.0427 - val_acc: 0.9909\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 93s 928ms/step - loss: 0.0210 - acc: 0.9915 - val_loss: 0.0500 - val_acc: 0.9960\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 91s 911ms/step - loss: 0.0186 - acc: 0.9965 - val_loss: 0.0576 - val_acc: 0.9950\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 92s 921ms/step - loss: 0.0123 - acc: 0.9970 - val_loss: 0.0366 - val_acc: 0.9960\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 92s 921ms/step - loss: 0.0069 - acc: 0.9965 - val_loss: 0.0259 - val_acc: 0.9960\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 92s 915ms/step - loss: 0.0076 - acc: 0.9970 - val_loss: 0.0292 - val_acc: 0.9960\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 92s 920ms/step - loss: 0.0132 - acc: 0.9975 - val_loss: 0.0842 - val_acc: 0.9950\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 93s 935ms/step - loss: 0.0126 - acc: 0.9980 - val_loss: 0.0309 - val_acc: 0.9960\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 91s 908ms/step - loss: 0.0113 - acc: 0.9990 - val_loss: 0.0651 - val_acc: 0.9960\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 91s 912ms/step - loss: 0.0072 - acc: 0.9990 - val_loss: 0.0851 - val_acc: 0.9838\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 91s 907ms/step - loss: 2.3728e-05 - acc: 1.0000 - val_loss: 0.0758 - val_acc: 0.9879\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 94s 937ms/step - loss: 0.0068 - acc: 0.9985 - val_loss: 0.0514 - val_acc: 0.9960\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 92s 917ms/step - loss: 0.0010 - acc: 0.9995 - val_loss: 0.0911 - val_acc: 0.9829\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 92s 915ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0740 - val_acc: 0.9869\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 92s 918ms/step - loss: 0.0071 - acc: 0.9990 - val_loss: 0.0681 - val_acc: 0.9909\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 91s 912ms/step - loss: 8.6247e-05 - acc: 1.0000 - val_loss: 0.1449 - val_acc: 0.9798\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 91s 908ms/step - loss: 0.0059 - acc: 0.9995 - val_loss: 0.0831 - val_acc: 0.9788\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 91s 906ms/step - loss: 1.2475e-06 - acc: 1.0000 - val_loss: 0.0658 - val_acc: 0.9960\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 93s 934ms/step - loss: 2.4079e-07 - acc: 1.0000 - val_loss: 0.0665 - val_acc: 0.9960\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 90s 905ms/step - loss: 1.3540e-07 - acc: 1.0000 - val_loss: 0.1211 - val_acc: 0.9829\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 92s 921ms/step - loss: 0.0145 - acc: 0.9975 - val_loss: 0.0715 - val_acc: 0.9960\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 92s 922ms/step - loss: 8.6491e-06 - acc: 1.0000 - val_loss: 0.0741 - val_acc: 0.9879\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 90s 904ms/step - loss: 0.0328 - acc: 0.9970 - val_loss: 0.1189 - val_acc: 0.9748\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 91s 912ms/step - loss: 2.1321e-04 - acc: 1.0000 - val_loss: 0.0832 - val_acc: 0.9950\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 93s 926ms/step - loss: 1.6873e-07 - acc: 1.0000 - val_loss: 0.0663 - val_acc: 0.9960\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 89s 895ms/step - loss: 0.0125 - acc: 0.9990 - val_loss: 0.1260 - val_acc: 0.9879\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 92s 916ms/step - loss: 2.2173e-06 - acc: 1.0000 - val_loss: 0.1013 - val_acc: 0.9778\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 89s 890ms/step - loss: 0.0408 - acc: 0.9970 - val_loss: 0.0764 - val_acc: 0.9838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loR6r9YZ2tuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkGu61Rhe14w",
        "colab_type": "text"
      },
      "source": [
        "### Using a pre trained model: Fine Tuning the pre trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuv6G51tqh8n",
        "colab_type": "text"
      },
      "source": [
        "Another widely used technique for model reuse, complementary to feature\n",
        "extraction, is fine-tuning (see figure 5.19). Fine-tuning consists of unfreezing a few of\n",
        "the top layers of a frozen model base used for feature extraction, and jointly training\n",
        "both the newly added part of the model (in this case, the fully connected classifier)\n",
        "and these top layers. This is called fine-tuning because it slightly adjusts the more\n",
        "abstract representations of the model being reused, in order to make them more relevant\n",
        "for the problem at hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2f19cAwq_dC",
        "colab_type": "text"
      },
      "source": [
        "Thus the\n",
        "steps for fine-tuning a network are as follow:\n",
        "1 Add your custom network on top of an already-trained base network.\n",
        "2 Freeze the base network.\n",
        "3 Train the part you added.\n",
        "4 Unfreeze some layers in the base network.\n",
        "5 Jointly train both these layers and the part you added.\n",
        "You already completed the first three steps when doing feature extraction. Let’s proceed\n",
        "with step 4: you’ll unfreeze your conv_base and then freeze individual layers\n",
        "inside it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n9D3Z2VrRim",
        "colab_type": "text"
      },
      "source": [
        "You’ll fine-tune the last three convolutional layers, which means all layers up to\n",
        "block4_pool should be frozen, and the layers block5_conv1, block5_conv2, and\n",
        "block5_conv3 should be trainable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zsdi8b8rSKq",
        "colab_type": "code",
        "outputId": "56acc092-16af-4827-8df0-5ce8fc58edb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "conv_base.trainable = True\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'block5_conv1':\n",
        "    set_trainable = True\n",
        "    \n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-025c4db58993>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconv_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mset_trainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconv_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'block5_conv1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mset_trainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'conv_base' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fMV9hwde79c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=30,\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NdwrCgEsbCz",
        "colab_type": "text"
      },
      "source": [
        "Here’s what you should take away from the exercises in the past two sections:\n",
        "\n",
        "Convnets are the best type of machine-learning models for computer-vision\n",
        "tasks. It’s possible to train one from scratch even on a very small dataset, with\n",
        "decent results.\n",
        "\n",
        "On a small dataset, overfitting will be the main issue. Data augmentation is a\n",
        "powerful way to fight overfitting when you’re working with image data.\n",
        "\n",
        "It’s easy to reuse an existing convnet on a new dataset via feature extraction.\n",
        "This is a valuable technique for working with small image datasets.\n",
        "\n",
        "As a complement to feature extraction, you can use fine-tuning, which adapts to\n",
        "a new problem some of the representations previously learned by an existing\n",
        "model. This pushes performance a bit further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huvEVNvcsaHf",
        "colab_type": "code",
        "outputId": "9db72f30-def0-49fb-9c5c-b9cbf56de867",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "divmod(23,5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr2n40Sg4_p5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}